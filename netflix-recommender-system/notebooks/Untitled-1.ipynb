{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import of libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "import logging\n",
    "import copy\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from netflix_recommender_system.logger import logger\n",
    "from netflix_recommender_system.config import config\n",
    "import netflix_recommender_system.server\n",
    "import netflix_recommender_system.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOMEPATH = ! echo $HOME\n",
    "# INPUT_FILE_PATH = pathlib.Path(HOMEPATH[0], \"projects/public-showcase/netflix-recommender-system-microservice/data\")\n",
    "# OUTPUT_DIRECTORY_PATH = pathlib.Path(HOMEPATH[0], \"projects/public-showcase/netflix-recommender-system-microservice/data\")\n",
    "# MODEL_DIRECTORY_PATH = pathlib.Path(HOMEPATH[0], \"projects/public-showcase/netflix-recommender-system-microservice/models\")\n",
    "\n",
    "\n",
    "# INPUT_DATA_DIRECTORY_PATH = pathlib.Path(\"..\", \"data\")\n",
    "# OUTPUT_DATA_DIRECTORY_PATH = pathlib.Path(\"..\", \"data\")\n",
    "# MODEL_DIRECTORY_PATH = pathlib.Path(\"..\", \"models\")\n",
    "# PREDICTION_TOTO_DICT = {\n",
    "#     \"rating_year\": 2005,\n",
    "#     \"avg_rating_of_similar_customers\": 3.0,\n",
    "#     \"number_of_similar_customers\": 1.0,\n",
    "#     \"rating_x_era_avg\": 3.2458677685950414,\n",
    "#     \"title\": \"Scotland\",\n",
    "#     \"release_era\": \"90s and 2000s\",\n",
    "# }\n",
    "# PREDICTION_DICT = {\n",
    "#     \"customer_id\": [1044034],\n",
    "#     # \"rating\": None,\n",
    "#     \"rating_date\": [\"2005-02-03\"],\n",
    "#     \"movie_id\": [12031],\n",
    "#     \"release_year\": [2002],\n",
    "#     \"title\": [\"Scotland\"], \n",
    "# }\n",
    "PREDICTION_DICT = {\n",
    "    \"customer_id\": 1044034,\n",
    "    # \"rating\": None,\n",
    "    \"rating_date\": \"2005-02-03\",\n",
    "    \"movie_id\": 12031,\n",
    "    \"release_year\": 2002,\n",
    "    \"title\": \"Scotland\", \n",
    "}\n",
    "\n",
    "MODE = \"prediction\" # \"prediction\" or \"training\"\n",
    "# MODEL_TIMESTAMP_ID = \"abc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = logging.getLogger(__name__)\n",
    "# logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomness seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocessing:\n",
    "    \"\"\"Preprocess.\"\"\"\n",
    "    def __init__(self, mode: str, input_datafilepath: str, output_datafilepath = None, prediction_dict = None):\n",
    "        if mode not in [\"training\", \"prediction\"]:\n",
    "            raise ValueError(\"Mode must be one of 'training' or 'prediction'\")\n",
    "        self.mode = mode\n",
    "        self.input_datafilepath = input_datafilepath\n",
    "        self.output_datafilepath = output_datafilepath\n",
    "        self.prediction_dict = prediction_dict\n",
    "\n",
    "    def __load_data_and_clean(self) -> pd.DataFrame:\n",
    "        \"\"\"Load input data file.\"\"\"\n",
    "        df = pd.read_csv(self.input_datafilepath)\n",
    "        df[\"rating_date\"] = pd.to_datetime(df[\"rating_date\"], format = \"mixed\")\n",
    "        df[\"rating_year\"] = pd.DatetimeIndex(df[\"rating_date\"]).year\n",
    "\n",
    "        def get_set(year):\n",
    "            if (year <= 2003):\n",
    "                return \"training\"\n",
    "            elif (year == 2004):\n",
    "                return \"validation\"\n",
    "            elif (year == 2005):\n",
    "                return \"prediction\"\n",
    "\n",
    "        df[\"set\"] = df[\"rating_year\"].apply(lambda x: get_set(x))\n",
    "        df = df[~(df[\"set\"] == \"prediction\")]\n",
    "\n",
    "        return df\n",
    "     \n",
    "\n",
    "    def __load_data_and_clean_and_insert_prediction_row(self)-> pd.DataFrame:\n",
    "        \"\"\"Insert prediction row.\"\"\"\n",
    "\n",
    "        df = self.__load_data_and_clean()\n",
    "\n",
    "        toto = self.prediction_dict[\"rating_date\"]\n",
    "        self.prediction_dict[\"rating_date\"] = datetime.datetime.strptime(toto, \"%Y-%m-%d\")\n",
    "\n",
    "        df = pd.concat(\n",
    "            [\n",
    "                df,\n",
    "                pd.DataFrame.from_dict(\n",
    "                    {\n",
    "                        \"customer_id\": [self.prediction_dict[\"customer_id\"]],\n",
    "                        \"rating_date\": [self.prediction_dict[\"rating_date\"]],\n",
    "                        \"movie_id\": [self.prediction_dict[\"movie_id\"]],\n",
    "                        \"release_year\": [self.prediction_dict[\"release_year\"]],\n",
    "                        \"title\": [self.prediction_dict[\"title\"]],\n",
    "                        \"rating\": [None],\n",
    "                        \"rating_year\": [2005],\n",
    "                        \"set\": [\"prediction\"]\n",
    "                    }\n",
    "                    )\n",
    "            ],\n",
    "            ignore_index = True\n",
    "        )\n",
    "\n",
    "        return df\n",
    "    \n",
    "    \n",
    "    def __write_data(self, df: pd.DataFrame):\n",
    "        \"\"\"Write features data file.\"\"\"\n",
    "        # df.to_csv(pathlib.Path(config[\"OUTPUT_DATA_DIRECTORY_PATH\"], \"netflix_prize_data_features.csv\"), sep=\",\", index = False)\n",
    "        df.to_csv(self.output_datafilepath, sep=\",\", index = False)\n",
    "    \n",
    "    \n",
    "    def compute_features(self):\n",
    "        \"\"\"Compute features.\"\"\"\n",
    "        if self.mode == \"training\":\n",
    "            df = self.__load_data_and_clean()\n",
    "        else:\n",
    "            df = self.__load_data_and_clean_and_insert_prediction_row()\n",
    "    \n",
    "        def get_era(year):\n",
    "            if (year < 1970):\n",
    "                return \"<1970\"\n",
    "            elif (year < 1990):\n",
    "                return \"70s and 80s\"\n",
    "            else:\n",
    "                return \"90s and 2000s\"\n",
    "\n",
    "        df[\"release_era\"] = df[\"release_year\"].apply(lambda x: get_era(x))\n",
    "\n",
    "        movie_id_list = set(df[\"movie_id\"])\n",
    "\n",
    "        counter = 0\n",
    "        for movie_id in movie_id_list:\n",
    "            ratings_import_movie_df = df[df[\"movie_id\"] == movie_id]\n",
    "            ratings_import_movie_df = ratings_import_movie_df.merge(ratings_import_movie_df, on=[\"movie_id\"], how=\"inner\")\n",
    "            ratings_import_movie_df = ratings_import_movie_df[~(ratings_import_movie_df.customer_id_x == ratings_import_movie_df.customer_id_y) & ~(ratings_import_movie_df.set_y == \"prediction\")]\n",
    "\n",
    "            if counter == 0:\n",
    "                ratings_feature_movie_df = ratings_import_movie_df\n",
    "            else:\n",
    "                ratings_feature_movie_df = pd.concat([ratings_feature_movie_df, ratings_import_movie_df])\n",
    "            print(f\"Movie with id {movie_id} was added. Dataset has currently {len(ratings_feature_movie_df)} rows.\")\n",
    "            counter +=1\n",
    "        \n",
    "        ratings_feature_movie_df[\"rating_gap\"] = abs(ratings_feature_movie_df[\"rating_x\"] - ratings_feature_movie_df[\"rating_y\"])\n",
    "\n",
    "        movie_id_list = set(df[\"movie_id\"])\n",
    "\n",
    "        counter = 0\n",
    "        for movie_id in movie_id_list:\n",
    "            cross_join_df = pd.merge(ratings_feature_movie_df[ratings_feature_movie_df[\"movie_id\"] == movie_id], ratings_feature_movie_df[(ratings_feature_movie_df[\"movie_id\"] != movie_id) & (ratings_feature_movie_df[\"set_y\"] == \"training\")].groupby([\"customer_id_x\", \"customer_id_y\"])[\"rating_gap\"].mean(), left_on=[\"customer_id_x\", \"customer_id_y\"], right_index=True, how = \"inner\", suffixes= [\"\", \"_avg\"])#.rename(columns={\"rating_gap_x\": \"avg_rating_gap\"})\n",
    "            avg_rating_of_similar_customers_movie_df = cross_join_df[cross_join_df[\"rating_gap_avg\"] <= .5].groupby([\"customer_id_x\", \"movie_id\"])[\"rating_y\"].mean().rename(\"avg_rating_of_similar_customers\")\n",
    "            number_of_similar_customers_movie_df = cross_join_df[cross_join_df[\"rating_gap_avg\"] <= .5].groupby([\"customer_id_x\", \"movie_id\"])[\"rating_y\"].size().rename(\"number_of_similar_customers\")\n",
    "            \n",
    "            rating_x_era_avg_movie_df = pd.merge(ratings_feature_movie_df[ratings_feature_movie_df[\"movie_id\"] == movie_id].drop_duplicates([\"customer_id_x\", \"movie_id\"]),\n",
    "                            ratings_feature_movie_df[(ratings_feature_movie_df[\"movie_id\"] != movie_id) & (ratings_feature_movie_df[\"set_x\"] == \"training\")].drop_duplicates([\"customer_id_x\", \"movie_id\"]).groupby([\"customer_id_x\", \"release_era_x\"])[\"rating_x\"].mean(), left_on=[\"customer_id_x\", \"release_era_x\"], right_index=True, how = \"inner\", suffixes= [\"\", \"_era_avg\"])[[\"customer_id_x\", \"movie_id\", \"rating_x_era_avg\"]]\n",
    "\n",
    "            if counter == 0:\n",
    "                avg_rating_of_similar_customers_df = avg_rating_of_similar_customers_movie_df\n",
    "                number_of_similar_customers_df = number_of_similar_customers_movie_df\n",
    "                rating_x_era_avg_df = rating_x_era_avg_movie_df\n",
    "            else:\n",
    "                avg_rating_of_similar_customers_df = pd.concat([avg_rating_of_similar_customers_df, avg_rating_of_similar_customers_movie_df])\n",
    "                number_of_similar_customers_df = pd.concat([number_of_similar_customers_df, number_of_similar_customers_movie_df])\n",
    "                rating_x_era_avg_df = pd.concat([rating_x_era_avg_df, rating_x_era_avg_movie_df])\n",
    "            counter +=1\n",
    "\n",
    "        ratings_features_export_df = pd.merge(df, avg_rating_of_similar_customers_df, left_on=[\"customer_id\", \"movie_id\"], right_index=True, how=\"left\").merge(number_of_similar_customers_df, left_on=[\"customer_id\", \"movie_id\"], right_index=True, how=\"left\").merge(rating_x_era_avg_df, left_on=[\"customer_id\", \"movie_id\"], right_on=[\"customer_id_x\", \"movie_id\"], how=\"left\").drop(columns = [\"customer_id_x\"])\n",
    "\n",
    "        avg_rating_of_similar_customers_imputed_value = ratings_features_export_df.loc[ratings_features_export_df[\"set\"] == \"training\", \"rating\"].mean(skipna=True)\n",
    "        number_of_similar_customers_imputed_value = 0 # technically not an imputed value\n",
    "        rating_x_era_avg_imputed_value = ratings_features_export_df.loc[ratings_features_export_df[\"set\"] == \"training\", \"rating\"].mean(skipna=True) # quick and dirty (would be better to have it by group)\n",
    "\n",
    "        ratings_features_export_df.loc[ratings_features_export_df[\"avg_rating_of_similar_customers\"].isna(), \"avg_rating_of_similar_customers\"] = avg_rating_of_similar_customers_imputed_value\n",
    "        ratings_features_export_df.loc[ratings_features_export_df[\"number_of_similar_customers\"].isna(), \"number_of_similar_customers\"] = number_of_similar_customers_imputed_value\n",
    "        ratings_features_export_df.loc[ratings_features_export_df[\"rating_x_era_avg\"].isna(), \"rating_x_era_avg\"] = rating_x_era_avg_imputed_value\n",
    "        \n",
    "        if self.mode == \"training\":\n",
    "            self.__write_data(df = ratings_features_export_df)\n",
    "        else:\n",
    "            ratings_features_export_df = ratings_features_export_df[ratings_features_export_df[\"set\"]== \"prediction\"]\n",
    "\n",
    "        return ratings_features_export_df\n",
    "\n",
    "#pathlib.Path(config[\"INPUT_DATA_DIRECTORY_PATH\"], \"data_sample.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline.compute_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline.prediction_dict[\"rating_date\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_import_df = pd.read_csv(pathlib.Path(config[\"INPUT_DATA_DIRECTORY_PATH\"], \"data_sample.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_import_df[\"rating_date\"] = pd.to_datetime(ratings_import_df[\"rating_date\"], format = \"mixed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_import_df[\"rating_year\"] = pd.DatetimeIndex(ratings_import_df[\"rating_date\"]).year\n",
    "\n",
    "def get_set(year):\n",
    "    if (year <= 2003):\n",
    "        return \"training\"\n",
    "    elif (year == 2004):\n",
    "        return \"validation\"\n",
    "    elif (year == 2005):\n",
    "        return \"prediction\"\n",
    "\n",
    "ratings_import_df[\"set\"] = ratings_import_df[\"rating_year\"].apply(lambda x: get_set(x))\n",
    "ratings_import_df = ratings_import_df[~(ratings_import_df[\"set\"] == \"prediction\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add prediction row if prediction mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (MODE == \"prediction\"):\n",
    "    PREDICTION_DICT[\"rating_date\"] = datetime.datetime.strptime(PREDICTION_DICT[\"rating_date\"], \"%Y-%m-%d\")\n",
    "\n",
    "    ratings_import_df = pd.concat(\n",
    "        [\n",
    "            ratings_import_df,\n",
    "            pd.DataFrame.from_dict(\n",
    "                {\n",
    "                    \"customer_id\": [PREDICTION_DICT[\"customer_id\"]],\n",
    "                    \"rating_date\": [PREDICTION_DICT[\"rating_date\"]],\n",
    "                    \"movie_id\": [PREDICTION_DICT[\"movie_id\"]],\n",
    "                    \"release_year\": [PREDICTION_DICT[\"release_year\"]],\n",
    "                    \"title\": [PREDICTION_DICT[\"title\"]],\n",
    "                    \"rating\": [None],\n",
    "                    \"rating_year\": [2005],\n",
    "                    \"set\": [\"prediction\"]\n",
    "                 }\n",
    "                )\n",
    "        ],\n",
    "        ignore_index = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_import_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_era(year):\n",
    "    if (year < 1970):\n",
    "        return \"<1970\"\n",
    "    elif (year < 1990):\n",
    "        return \"70s and 80s\"\n",
    "    else:\n",
    "        return \"90s and 2000s\"\n",
    "\n",
    "ratings_import_df[\"release_era\"] = ratings_import_df[\"release_year\"].apply(lambda x: get_era(x))\n",
    "\n",
    "movie_id_list = set(ratings_import_df[\"movie_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_import_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for movie_id in movie_id_list:\n",
    "    ratings_import_movie_df = ratings_import_df[ratings_import_df[\"movie_id\"] == movie_id]\n",
    "    ratings_import_movie_df = ratings_import_movie_df.merge(ratings_import_movie_df, on=[\"movie_id\"], how=\"inner\")\n",
    "    ratings_import_movie_df = ratings_import_movie_df[~(ratings_import_movie_df.customer_id_x == ratings_import_movie_df.customer_id_y) & ~(ratings_import_movie_df.set_y == \"prediction\")]\n",
    "\n",
    "    if counter == 0:\n",
    "        ratings_feature_movie_df = ratings_import_movie_df\n",
    "    else:\n",
    "        ratings_feature_movie_df = pd.concat([ratings_feature_movie_df, ratings_import_movie_df])\n",
    "    print(f\"Movie with id {movie_id} was added. Dataset has currently {len(ratings_feature_movie_df)} rows.\")\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_feature_movie_df[\"rating_gap\"] = abs(ratings_feature_movie_df[\"rating_x\"] - ratings_feature_movie_df[\"rating_y\"])\n",
    "ratings_feature_movie_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_id_list = set(ratings_import_df[\"movie_id\"])\n",
    "\n",
    "counter = 0\n",
    "for movie_id in movie_id_list:\n",
    "    cross_join_df = pd.merge(ratings_feature_movie_df[ratings_feature_movie_df[\"movie_id\"] == movie_id], ratings_feature_movie_df[(ratings_feature_movie_df[\"movie_id\"] != movie_id) & (ratings_feature_movie_df[\"set_y\"] == \"training\")].groupby([\"customer_id_x\", \"customer_id_y\"])[\"rating_gap\"].mean(), left_on=[\"customer_id_x\", \"customer_id_y\"], right_index=True, how = \"inner\", suffixes= [\"\", \"_avg\"])#.rename(columns={\"rating_gap_x\": \"avg_rating_gap\"})\n",
    "    avg_rating_of_similar_customers_movie_df = cross_join_df[cross_join_df[\"rating_gap_avg\"] <= .5].groupby([\"customer_id_x\", \"movie_id\"])[\"rating_y\"].mean().rename(\"avg_rating_of_similar_customers\")\n",
    "    number_of_similar_customers_movie_df = cross_join_df[cross_join_df[\"rating_gap_avg\"] <= .5].groupby([\"customer_id_x\", \"movie_id\"])[\"rating_y\"].size().rename(\"number_of_similar_customers\")\n",
    "    \n",
    "    rating_x_era_avg_movie_df = pd.merge(ratings_feature_movie_df[ratings_feature_movie_df[\"movie_id\"] == movie_id].drop_duplicates([\"customer_id_x\", \"movie_id\"]),\n",
    "                    ratings_feature_movie_df[(ratings_feature_movie_df[\"movie_id\"] != movie_id) & (ratings_feature_movie_df[\"set_x\"] == \"training\")].drop_duplicates([\"customer_id_x\", \"movie_id\"]).groupby([\"customer_id_x\", \"release_era_x\"])[\"rating_x\"].mean(), left_on=[\"customer_id_x\", \"release_era_x\"], right_index=True, how = \"inner\", suffixes= [\"\", \"_era_avg\"])[[\"customer_id_x\", \"movie_id\", \"rating_x_era_avg\"]]\n",
    "\n",
    "    if counter == 0:\n",
    "        avg_rating_of_similar_customers_df = avg_rating_of_similar_customers_movie_df\n",
    "        number_of_similar_customers_df = number_of_similar_customers_movie_df\n",
    "        rating_x_era_avg_df = rating_x_era_avg_movie_df\n",
    "    else:\n",
    "        avg_rating_of_similar_customers_df = pd.concat([avg_rating_of_similar_customers_df, avg_rating_of_similar_customers_movie_df])\n",
    "        number_of_similar_customers_df = pd.concat([number_of_similar_customers_df, number_of_similar_customers_movie_df])\n",
    "        rating_x_era_avg_df = pd.concat([rating_x_era_avg_df, rating_x_era_avg_movie_df])\n",
    "    counter +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_x_era_avg_movie_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_features_export_df = pd.merge(ratings_import_df, avg_rating_of_similar_customers_df, left_on=[\"customer_id\", \"movie_id\"], right_index=True, how=\"left\").merge(number_of_similar_customers_df, left_on=[\"customer_id\", \"movie_id\"], right_index=True, how=\"left\").merge(rating_x_era_avg_df, left_on=[\"customer_id\", \"movie_id\"], right_on=[\"customer_id_x\", \"movie_id\"], how=\"left\").drop(columns = [\"customer_id_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_rating_of_similar_customers_imputed_value = ratings_features_export_df.loc[ratings_features_export_df[\"set\"] == \"training\", \"rating\"].mean(skipna=True)\n",
    "number_of_similar_customers_imputed_value = 0 # technically not an imputed value\n",
    "rating_x_era_avg_imputed_value = ratings_features_export_df.loc[ratings_features_export_df[\"set\"] == \"training\", \"rating\"].mean(skipna=True) # quick and dirty (would be better to have it by group)\n",
    "\n",
    "ratings_features_export_df.loc[ratings_features_export_df[\"avg_rating_of_similar_customers\"].isna(), \"avg_rating_of_similar_customers\"] = avg_rating_of_similar_customers_imputed_value\n",
    "ratings_features_export_df.loc[ratings_features_export_df[\"number_of_similar_customers\"].isna(), \"number_of_similar_customers\"] = number_of_similar_customers_imputed_value\n",
    "ratings_features_export_df.loc[ratings_features_export_df[\"rating_x_era_avg\"].isna(), \"rating_x_era_avg\"] = rating_x_era_avg_imputed_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_features_export_df.to_csv(pathlib.Path(config[\"OUTPUT_DATA_DIRECTORY_PATH\"], \"netflix_prize_data_features.csv\"), sep=\",\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_features_export_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_features_export_df[ratings_features_export_df[\"set\"]== \"prediction\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "# #features = [\"title\", \"release_era\", \"rating_year\", \"avg_rating_of_similar_customers\", \"number_of_similar_customers\", \"rating_x_era_avg\"]\n",
    "# categorical_features = [\"title\", \"release_era\"]\n",
    "# numeric_features = [\"rating_year\", \"avg_rating_of_similar_customers\", \"number_of_similar_customers\", \"rating_x_era_avg\"]\n",
    "# numeric_features_np = ratings_features_export_df[numeric_features].to_numpy()\n",
    "# categorical_features_np = ratings_features_export_df[categorical_features].to_numpy()\n",
    "# encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "# encoder.fit(toto)\n",
    "# categorical_features_onehotencoded_np = encoder.transform(toto).toarray()\n",
    "\n",
    "# y_np = ratings_features_export_df[\"rating\"].to_numpy()\n",
    "# #encoder.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SupervisedLearning:\n",
    "    \"\"\"Actions for supervised learning tasks, only RandomForestRegression is supported as of now.\"\"\"\n",
    "    def __init__(self,\n",
    "                 training_df: pd.DataFrame,\n",
    "                 validation_df: pd.DataFrame,\n",
    "                 numeric_features_names: list[str],\n",
    "                 categorical_features_names: list[str],\n",
    "                 y_name: str,\n",
    "                 seed: int = 0) -> None:\n",
    "        \"\"\"Instantiate object.\"\"\"\n",
    "        self.training_df = training_df\n",
    "        self.validation_df = validation_df\n",
    "        self.numeric_features_names = numeric_features_names\n",
    "        self.categorical_features_names = categorical_features_names\n",
    "        self.y_name = y_name\n",
    "        self.seed = seed\n",
    "\n",
    "    def create_training_arrays(self) -> None:\n",
    "        \"\"\"Create training arrays.\"\"\"\n",
    "        self.numeric_features_training = self.training_df[self.numeric_features_names].to_numpy()\n",
    "        self.categorical_features_training = self.training_df[self.categorical_features_names].to_numpy()\n",
    "        self.encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "        self.encoder.fit(self.categorical_features_training)\n",
    "        self.encoded_categorical_features_training = self.encoder.transform(self.categorical_features_training).toarray()\n",
    "        self.X_training = np.concatenate((self.numeric_features_training, self.encoded_categorical_features_training), axis = 1)\n",
    "        self.y_training = self.training_df[self.y_name].to_numpy()\n",
    "    \n",
    "    def create_validation_arrays(self) -> None:\n",
    "        \"\"\"Create validation arrays.\"\"\"\n",
    "        self.numeric_features_validation = self.validation_df[self.numeric_features_names].to_numpy()\n",
    "        self.categorical_features_validation = self.validation_df[self.categorical_features_names].to_numpy()\n",
    "        self.encoded_categorical_features_validation = self.encoder.transform(self.categorical_features_validation).toarray()\n",
    "        self.X_validation = np.concatenate((self.numeric_features_validation, self.encoded_categorical_features_validation), axis = 1)\n",
    "        self.y_validation = self.validation_df[self.y_name].to_numpy()\n",
    "    \n",
    "    def train(self) -> None:\n",
    "        \"\"\"Train model.\"\"\"\n",
    "        self.model = RandomForestRegressor(max_depth=1, max_features = 2, random_state=self.seed)\n",
    "        self.model.fit(self.X_training, self.y_training)\n",
    "        self.rmse_training = mean_squared_error(\n",
    "            self.y_training,\n",
    "            self.model.predict(self.X_training),\n",
    "        )\n",
    "\n",
    "    def validate(self) -> None:\n",
    "        \"\"\"Pass.\"\"\"\n",
    "        self.rmse_validation = mean_squared_error(\n",
    "            self.y_validation,\n",
    "            self.model.predict(self.X_validation),\n",
    "        )\n",
    "\n",
    "    def predict(self, prediction_df: pd.DataFrame) -> np.ndarray:\n",
    "        \"\"\"Pass.\"\"\"\n",
    "        self.numeric_features_prediction = prediction_df[self.numeric_features_names].to_numpy()\n",
    "        self.categorical_features_prediction = prediction_df[self.categorical_features_names].to_numpy()\n",
    "        self.encoded_categorical_features_prediction = self.encoder.transform(self.categorical_features_prediction).toarray()\n",
    "        self.X_prediction = np.concatenate((self.numeric_features_prediction, self.encoded_categorical_features_prediction), axis = 1)\n",
    "        return self.model.predict(self.X_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features_np = np.concatenate((categorical_features_onehotencoded_np, numeric_features_np), axis = 1)\n",
    "# with open(\"./toto.pickle\", \"wb\") as f:\n",
    "#     pickle.dump(SupervisedLearning, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"training\":\n",
    "\n",
    "    MODEL_TIMESTAMP_ID_NEW = str(time.time()).split(\".\")[0]\n",
    "\n",
    "    os.makedirs(pathlib.Path(config[\"MODEL_DIRECTORY_PATH\"], MODEL_TIMESTAMP_ID_NEW))\n",
    "\n",
    "    preprocessing_pipeline = Preprocessing(\n",
    "        mode = \"training\",\n",
    "        input_datafilepath = pathlib.Path(config[\"DATA_DIRECTORY_PATH\"], \"data_sample.csv\"),\n",
    "        output_datafilepath = pathlib.Path(config[\"MODEL_DIRECTORY_PATH\"], MODEL_TIMESTAMP_ID_NEW, \"training_dataset_features.csv\"),\n",
    "    )\n",
    "    \n",
    "    df = preprocessing_pipeline.compute_features()\n",
    "\n",
    "    learning_pipeline = SupervisedLearning(\n",
    "        training_df = df[df[\"set\"] == \"training\"],\n",
    "        validation_df = df[df[\"set\"] == \"validation\"],\n",
    "        numeric_features_names = [\"rating_year\", \"avg_rating_of_similar_customers\", \"number_of_similar_customers\", \"rating_x_era_avg\"],\n",
    "        categorical_features_names = [\"title\", \"release_era\"],\n",
    "        y_name = \"rating\",\n",
    "        seed = config[\"SEED\"],\n",
    "    )\n",
    "\n",
    "    learning_pipeline.create_training_arrays()\n",
    "\n",
    "    learning_pipeline.create_validation_arrays()\n",
    "\n",
    "    learning_pipeline.train()\n",
    "\n",
    "    learning_pipeline.validate()\n",
    "\n",
    "    logger.info(f\"RMSE on training set is: {learning_pipeline.rmse_training}\")\n",
    "    logger.info(f\"RMSE on validation set is: {learning_pipeline.rmse_validation}\")\n",
    "\n",
    "    summary = {\n",
    "        \"rmse_training\": learning_pipeline.rmse_training,\n",
    "        \"rmse_validation\": learning_pipeline.rmse_validation,\n",
    "        }\n",
    "    \n",
    "    \n",
    "    with open(pathlib.Path(config[\"MODEL_DIRECTORY_PATH\"], MODEL_TIMESTAMP_ID_NEW, \"model.pickle\"), \"wb\") as file:\n",
    "        pickle.dump(learning_pipeline, file)\n",
    "\n",
    "    # ratings_features_export_df.to_csv(pathlib.Path(config[\"MODEL_DIRECTORY_PATH\"], MODEL_TIMESTAMP_ID_NEW, \"training_dataset_features.csv\"), sep=\",\", index = False)\n",
    "\n",
    "    with open(pathlib.Path(config[\"MODEL_DIRECTORY_PATH\"], MODEL_TIMESTAMP_ID_NEW, \"summary.yaml\"), \"w\") as file:\n",
    "        yaml.dump(summary, file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if MODE == \"prediction\":\n",
    "    \n",
    "    with open(pathlib.Path(config[\"MODEL_DIRECTORY_PATH\"], str(config[\"MODEL_TIMESTAMP_ID\"]), \"model.pickle\"), \"rb\") as file:\n",
    "        learning_pipeline = pickle.load(file)\n",
    "    \n",
    "    preprocessing_pipeline = Preprocessing(\n",
    "        mode = \"prediction\",\n",
    "        input_datafilepath = pathlib.Path(config[\"DATA_DIRECTORY_PATH\"], \"data_sample.csv\"),\n",
    "        prediction_dict=copy.deepcopy(PREDICTION_DICT), # deep copy to prevent the side-effect change of dict argument inside the function\n",
    "    )\n",
    "\n",
    "    logger.info(\n",
    "        f\"Prediction is: {learning_pipeline.predict(prediction_df = learning_pipeline.predict(prediction_df=preprocessing_pipeline.compute_features()))}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(pathlib.Path(config[\"MODEL_DIRECTORY_PATH\"], str(config[\"MODEL_TIMESTAMP_ID\"]), \"model.pickle\"), \"rb\") as file:\n",
    "    learning_pipeline = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.SupervisedLearning at 0x79b31677aeb0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_pipeline = Preprocessing(\n",
    "    mode = \"prediction\",\n",
    "    input_datafilepath = pathlib.Path(config[\"DATA_DIRECTORY_PATH\"], \"data_sample.csv\"),\n",
    "    prediction_dict=copy.deepcopy(PREDICTION_DICT), # deep copy to prevent the side-effect change of dict argument inside the function\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movie with id 14146 was added. Dataset has currently 8010 rows.\n",
      "Movie with id 10403 was added. Dataset has currently 11670 rows.\n",
      "Movie with id 16676 was added. Dataset has currently 16640 rows.\n",
      "Movie with id 2505 was added. Dataset has currently 23446 rows.\n",
      "Movie with id 586 was added. Dataset has currently 23866 rows.\n",
      "Movie with id 6953 was added. Dataset has currently 24516 rows.\n",
      "Movie with id 1132 was added. Dataset has currently 26586 rows.\n",
      "Movie with id 16365 was added. Dataset has currently 31278 rows.\n",
      "Movie with id 12942 was added. Dataset has currently 34034 rows.\n",
      "Movie with id 5231 was added. Dataset has currently 35516 rows.\n",
      "Movie with id 5488 was added. Dataset has currently 41996 rows.\n",
      "Movie with id 17743 was added. Dataset has currently 44752 rows.\n",
      "Movie with id 6162 was added. Dataset has currently 48658 rows.\n",
      "Movie with id 12031 was added. Dataset has currently 54899 rows.\n",
      "Movie with id 8469 was added. Dataset has currently 57551 rows.\n",
      "Movie with id 10903 was added. Dataset has currently 58883 rows.\n",
      "Movie with id 10041 was added. Dataset has currently 65689 rows.\n",
      "Movie with id 10111 was added. Dataset has currently 75195 rows.\n"
     ]
    }
   ],
   "source": [
    "toto = preprocessing_pipeline.compute_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning_pipeline.prediction_df#[numeric_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customer_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>rating_date</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>release_year</th>\n",
       "      <th>title</th>\n",
       "      <th>rating_year</th>\n",
       "      <th>set</th>\n",
       "      <th>release_era</th>\n",
       "      <th>avg_rating_of_similar_customers</th>\n",
       "      <th>number_of_similar_customers</th>\n",
       "      <th>rating_x_era_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1105</th>\n",
       "      <td>1044034</td>\n",
       "      <td>None</td>\n",
       "      <td>2005-02-03</td>\n",
       "      <td>12031</td>\n",
       "      <td>2002</td>\n",
       "      <td>Scotland</td>\n",
       "      <td>2005</td>\n",
       "      <td>prediction</td>\n",
       "      <td>90s and 2000s</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.261954</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      customer_id rating rating_date  movie_id  release_year     title  \\\n",
       "1105      1044034   None  2005-02-03     12031          2002  Scotland   \n",
       "\n",
       "      rating_year         set    release_era avg_rating_of_similar_customers  \\\n",
       "1105         2005  prediction  90s and 2000s                             3.0   \n",
       "\n",
       "      number_of_similar_customers rating_x_era_avg  \n",
       "1105                          1.0         3.261954  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3.24585137])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "learning_pipeline.predict(prediction_df=toto)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netflix-recommender-system-microservice",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
